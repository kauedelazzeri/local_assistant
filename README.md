# InsightFinder â€” Busca Inteligente por Arquivos Locais

Uma aplicaÃ§Ã£o local que permite buscar informaÃ§Ãµes em linguagem natural dentro de arquivos do seu computador, funcionando como um "Copilot pessoal" para seus arquivos.

## ğŸ“¸ Interface da AplicaÃ§Ã£o

A InsightFinder oferece uma interface web intuitiva e moderna para realizar buscas inteligentes em seus arquivos locais. Veja como ela funciona:

### PÃ¡gina Inicial
![PÃ¡gina Inicial](images/2025-05-03%2017_36_50-.png)
*A interface inicial permite inserir o termo de busca, selecionar a pasta e escolher os tipos de arquivo a serem pesquisados.*

### Resultados da Busca
![Resultados Iniciais](images/2025-05-03%2017_57_02-.png)
*ApÃ³s iniciar a busca, a aplicaÃ§Ã£o mostra os resultados em tempo real, com a IA (Llama2) analisando cada arquivo e indicando sua relevÃ¢ncia para o termo pesquisado.*

### Busca AvanÃ§ada
![Busca AvanÃ§ada](images/2025-05-03%2017_57_09-.png)
*Conforme a busca progride, mais arquivos sÃ£o analisados e exibidos, permitindo uma visÃ£o completa dos documentos relevantes encontrados em seu sistema.*

A interface foi projetada para ser simples e eficiente, com:
- Campo de busca em linguagem natural
- SeleÃ§Ã£o de pasta personalizada
- Filtros por tipo de arquivo
- Resultados em tempo real
- Indicadores visuais de relevÃ¢ncia
- BotÃ£o para abrir arquivos diretamente
- Justificativas detalhadas da IA para cada resultado

## ğŸš€ Funcionalidades

- Busca em linguagem natural
- Suporte a mÃºltiplos formatos de arquivo (TXT, PDF)
    - PPTX, XLSX, JPG, PNG em breve
- Processamento local com IA via Ollama
- Interface web amigÃ¡vel
- Privacidade total (tudo roda localmente)

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Node.js 16+ (para o frontend)
- Ollama instalado e rodando localmente
- Tesseract OCR instalado (para processamento de imagens)

## ğŸ› ï¸ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/seu-usuario/local_assistant.git
cd local_assistant
```
2. Crie e ative um ambiente virtual:
# Windows (cmd) ou PowerShell
```bash
python -m venv venv
venv\Scripts\activate
```
3. Instale as dependÃªncias do backend dentro do ambiente virutal:
```bash
cd backend
pip install -r requirements.txt
```
4. Instale as dependÃªncias do frontend em outro terminal:
```bash
cd frontend
npm install
```

## âš™ï¸ Como usar IA local com Ollama

1. Baixe e instale o [Ollama](https://ollama.com/download).
2. ApÃ³s instalar, rode em outro terminal:
```bash
ollama run llama2
```
3. Para usar outro modelo, como `phi`, `mistral`, ou `llava`, execute:
```bash
ollama run nome-do-modelo
```
4. Para trocar o modelo padrÃ£o no cÃ³digo, edite o arquivo:
```bash
backend/main.py
```
E altere a linha:
```python
OLLAMA_MODEL = "llama2"
```

## ğŸš€ Executando

1. Ative o ambiente virtual (caso ainda nÃ£o esteja ativo):
# Windows (cmd)
```bash
venv\Scripts\activate
```
2. Inicie o backend com FastAPI no ambiente virtual:
```python
cd backend
uvicorn main:app --reload
```

3. Em outro terminal, inicie o frontend:
```bash
cd frontend
npm run start
```
4. Em outro terminal, inicie o servidor llama2:
```bash
ollama serve
```

4. Acesse a aplicaÃ§Ã£o em `http://localhost:3000`, o node deve abrir automaticamente no navegador

## ğŸ“ Estrutura do Projeto

```
insight-finder/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ images/
â””â”€â”€ README.md
```

## ğŸ“‚ Caminho padrÃ£o de busca

O projeto tenta automaticamente definir a pasta padrÃ£o de busca como a pasta de Downloads do sistema (Windows). O campo pode ser editado manualmente na interface.

## ğŸ”’ Privacidade

- Todo o processamento Ã© feito localmente
- Nenhum dado Ã© enviado para a internet
- Os arquivos sÃ£o indexados apenas nas pastas autorizadas

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
